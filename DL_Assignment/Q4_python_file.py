# -*- coding: utf-8 -*-
"""Q4_Text_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iUQZpxp3Of58CP_b9JkkqH60EPNXiERA
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer
import torch
from torch.utils.data import Dataset
from transformers import BertForSequenceClassification
from transformers import Trainer, TrainingArguments

# Sample dataset
data = {
    'text': [
        "Wow!, this is an IITG internship",
        "This is the worst experience ever.",
        "It's okay, not great.",
        "Absolutely fantastic service.",
        "I'm not satisfied with the quality.",
        "The item is average.",
        "I love this product!",
        "I am not happy with it",
        "It's okay, nothing special.",
        "This is an amazing product! Highly recommend.",
        "I am so disappointed with the service.",
        "The weather is quite neutral today, neither good nor bad.",
        "Loved every bit of it, truly fantastic!",
        "This movie was terrible, a complete waste of time.",
        "It's okay, nothing special.",
        "Excellent customer support!",
        "Very poor quality, I regret buying it.",
        "The news report was unbiased and factual.",
        "Absolutely brilliant, couldn't be happier!",
        "I have mixed feelings about this, somewhat confusing.",
        "What a horrible experience.",
        "The food was decent.",
        "Such a wonderful day!",
        "I'm feeling indifferent.",
        "This service is perfect!",
        "Extremely frustrating situation.",
        "The article presented a balanced view."
    ],
    'label': ['positive',
              'negative',
              'neutral',
              'positive',
              'negative',
              'neutral',
              'positive',
              'negative',
              'neutral',
              'positive',
              'negative',
              'neutral',
              'positive',
              'negative',
              'neutral',
              'positive',
              'negative',
              'neutral',
              'positive',
              'neutral',
              'negative',
              'neutral',
              'positive',
              'neutral',
              'positive',
              'negative',
              'neutral'
              ]
}

df = pd.DataFrame(data)

label_encoder = LabelEncoder()
df['label_id'] = label_encoder.fit_transform(df['label'])

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
encodings = tokenizer(list(df['text']), truncation=True, padding=True, return_tensors='pt')

class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

dataset = SentimentDataset(encodings, list(df['label_id']))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3).to(device)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=40,
    per_device_train_batch_size=2,
    logging_dir='./logs',
    logging_steps=10,
    eval_strategy="no",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset
)

trainer.train()

def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    # Move input tensors to the same device as the model
    inputs = {key: val.to(model.device) for key, val in inputs.items()}
    outputs = model(**inputs)
    pred = torch.argmax(outputs.logits, dim=1).item()
    return label_encoder.inverse_transform([pred])[0]

# Test examples
print(predict_sentiment("This is the best product I've used!"))  # Expected: positive
print(predict_sentiment("Wow! its an internship at IITG"))        # Expected: positive
print(predict_sentiment("It doesn't work at all."))              # Expected: negative
print(predict_sentiment("It's okay, nothing special."))          # Expected: neutral

